## 1. 复习

$p(o|c)=\cfrac{exp(u_0^Tv_c)}{\sum_{w\in{v}}exp(u_w^Tv_c)}$

## 2. 优化：梯度下降法

* 我们需要目标函数最小化

* 梯度下降法用来最小化目标函数

* 方法：对于当前值$\theta$，计算梯度$J(\theta)$,沿着下降的方向不断减小梯度

  ![image-20200218154551787](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200218154551787.png)

* 更新公式：

  * 矩阵版

  $$
  \theta^{new}=\theta^{old}-\alpha\nabla_\theta J(\theta)
  $$

  ​		其中$\alpha$称为步长或者学习率

  * 对单个变量而言：
    $$
    \theta_j^{new}=\theta_j^{old}-\alpha \cfrac{\partial}{\partial\theta_j^{old}}J(\theta)
    $$

  * 代码实现

    ```python
    while True:
      theta_grad =evaluate_gradient(J,window,theta)
      theta = theta -alpha*theta_grad
    ```

  ### (SGD)随机梯度下降

  * 梯度下降的缺点：$J(\theta)$是一个针对语料库中所有参数的函数，所以对每一个$\theta$计算$\nabla_{\theta}J(\theta)$会浪费很多时间

  * SGD:每次重复相同的窗口并更新参数

    ```python
    while True:
      window = sample_window(corpus) #corpus:语料库
      theta_grad =evaluate_gradient(J,window,theta)
      theta = theta -alpha*theta_grad
    ```

  ### 词向量的随机梯度

  * 由于每个窗口里面的最多有$2m+1$个单词，所以求$\nabla_\theta J_t(\theta)$是十分简单的
  * $m$为你在句子中所选择的向前和向后所要考虑的单词数量
  * 我们只需要更新实际出现的词向量参数

  ### Word2vec:More details

  * 两个向量更方便优化
    * 实际上通过对两个向量的取平均可以使得每个单词只需要一个向量

  #### 两个基础模型

  * **Skip-gram(SG**):根据中心词预测上下文
  * **Continuous Bag of Words(CBOW)**：根据上下文预测中心词

  #### 优化训练过程方法：

  * 负采样(Negative smpling)

  note：朴素贝叶斯训练时更简单也更加耗时

###  基于负采样的skip-gram模型(HW2)

* $p(o|c)=\cfrac{exp(u_0^Tv_c)}{\sum_{w\in{v}}exp(u_w^Tv_c)}$

* 主要思路：训练一个正确的搭配（中心词和上下文窗口中的词）与几个噪声对（中心词与一个随机词配对）的二元逻辑回归

* 最大化目标函数？：
  $$
  J(\theta)=\cfrac{1}{T}\sum_{t=1}^{T}J_t(\theta)\\
  J_t(\theta)=log\sigma(u_o^Tv_c)+\sum_{i=1}^{k}E_{j\backsim P(w)}[log\sigma(-u_j^Tv_c)]
  $$

* Sigmoid funcition:$\sigma(x)=\cfrac{1}{1+e^{-x}}$

<img src="/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200218233030148.png" alt="image-20200218233030148" style="zoom:50%;" />

#### 采用了负采样后的目标函数

![image-20200218235138310](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200218235138310.png)

* 取前k个负采样（根据单词概率）
* 最大化出现真实外部单词的可能性，最小化在中心词周围出现随机词的概率
* $P(w)=\cfrac{U(w)^{3/4}}{Z}$
  * $U(w)$:unigram 分布的$3/4$次方会降低高频词的概率，提高低频词的概率
  * $Z$:所有词的总数 *???*

## 3. 为什么不直接采用词组计数？

当采用共生矩阵计数时

#### 窗口 vs 整个文档

* 窗口：类似于word2vec，对每个单词进行窗口滚动--->捕捉语法（POS)和语义分析
* 整个文档：文档的词组矩阵会因为主题而导致潜在的语义分析

#### Example:基于共生矩阵

* 窗口长度：1
* 对称：不管窗口是在左边还是右边
* 语料库
  * I like deep learing.
  * I like NLP.
  * I enjoy flying.

共生矩阵：

![image-20200219004221682](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200219004221682.png)

#### 基于共生矩阵的缺点

* 矩阵大小会随着词汇量的增长而增长
* 非常高的维度会导致浪费很多的内存
* 后续分类模型存在稀疏性问题导致模型不够**健壮**

#### 解决方法：降低矩阵维度

* 思路：通过对稀疏矩阵的分解获得代表了主要的信息的密集矩阵

#### 方法1：提取原始矩阵的主要信息**(HW1)**

对X进行矩阵分解
$$
X=U\varSigma V^T
$$
其中U和V都是非奇异的

![image-20200219200054729](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200219200054729.png)

![image-20200220140137195](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220140137195.png)

![image-20200220140154875](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220140154875.png)

#### 处理高频词（比如the，he，has）

* 例如：min(X,100)
* 使窗口覆盖更多的词
* 使用皮尔逊相关系数代替计数，并将负值设置为0

### 共生计数 VS 直接预测

![image-20200220150921688](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220150921688.png)

比较SVD这种count based模型与Word2Vec这种direct prediction模型，它们各有优缺点：

* Count based模型优点是训练快速，并且有效的利用了统计信息，缺点是对于高频词汇较为偏向，并且仅能概括词组的相关性，而且有的时候产生的word vector对于解释词的含义如word analogy等任务效果不好；
* Direct Prediction优点是可以概括比相关性更为复杂的信息，进行word analogy等任务时效果较好，缺点是对统计信息利用的不够充分。所以Manning教授他们想采取一种方法可以结合两者的优势，并将这种算法命名为GloVe（Global Vectors的缩写），表示他们可以有效的利用全局的统计信息。

### 同现概率的比率可以编码含义成分

![image-20200220151839611](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220151839611.png)

![image-20200220151849447](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220151849447.png)

例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，它们之间的关系可通过与不同的单词$x$的co-occurrence probability 的比值来描述，例如对于solid固态，虽然 $P(solid|ice)$ 与 $P(solid|steam)$ 本身很小，不能透露有效的信息，但是它们的**比值** $\cfrac{P(solid|ice)}{P(solid|steam)}$却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大，对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则**比值接近于1**。

#### 结论

相较于单纯的co-occurrence probability，实际上co-occurrence probability的相对比值更有意义。

#### 在单词向量空间中捕获共现概率的比率作为线性含义成分

利用向量差异的log双线性模型
$$
w_i*w_j=logP(i|j)\\w_x*(w_a-w_b)=log\cfrac{P(x|a)}{P(x|b)}
$$

#### [公式推导过程](https://zhuanlan.zhihu.com/p/60208480)：

基于对于以上概率比值的观察，我们假设模型的函数有如下形式：

![[公式]](https://www.zhihu.com/equation?tex=F%28w_i%2Cw_j%2C%5Ctilde+w_k%29+%3D+%5Cfrac%7BP_%7Bik%7D%7D%7BP_%7Bjk%7D%7D)

其中， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D) 代表了context vector，如上例中的solid，gas，water，fashion等。 ![[公式]](https://www.zhihu.com/equation?tex=w_i%2Cw_j) 则是我们要比较的两个词汇，如上例中的ice，steam。

![[公式]](https://www.zhihu.com/equation?tex=F)的可选的形式过多，我们希望有所限定。首先我们希望的是 ![[公式]](https://www.zhihu.com/equation?tex=F) 能有效的在单词向量空间内表示概率比值，由于向量空间是线性空间，一个自然的假设是 ![[公式]](https://www.zhihu.com/equation?tex=F) 是关于向量 ![[公式]](https://www.zhihu.com/equation?tex=w_i%2Cw_j) 的差的形式：

![[公式]](https://www.zhihu.com/equation?tex=F%28w_i-w_j%2C%5Ctilde+w_k%29+%3D+%5Cfrac%7BP_%7Bik%7D%7D%7BP_%7Bjk%7D%7D)

等式右边为标量形式，左边如何操作能将矢量转化为标量形式呢？一个自然的选择是矢量的点乘形式：

$$
F((w_i-w_j)^T\tilde{w}_k)=\cfrac{P_{ik}}{P_{jk}}
$$


在此，作者又对其进行了对称性分析，即对于word-word co-occurrence，将向量划分为center word还是context word的选择是不重要的，即我们在交换 ![[公式]](https://www.zhihu.com/equation?tex=w%5Cleftrightarrow+%5Ctilde+w) 与 ![[公式]](https://www.zhihu.com/equation?tex=X%5Cleftrightarrow+X%5ET) 的时候该式仍然成立。如何保证这种对称性呢？

我们分两步来进行，首先要求满足 ![[公式]](https://www.zhihu.com/equation?tex=F%28%28w_i-w_j%29%5ET%5Ctilde+w_k%29+%3D+%5Cfrac%7BF%28w_i%5ET%5Ctilde+w_k%29%7D%7BF%28w_j%5ET%5Ctilde+w_k%29%7D) ，该方程的解为 ![[公式]](https://www.zhihu.com/equation?tex=F%3Dexp) 同时与$F((w_i-w_j)^T\tilde{w}_k)=\cfrac{P_{ik}}{P_{jk}}$相比较有 ![[公式]](https://www.zhihu.com/equation?tex=F%28w_i%5ET%5Ctilde+w_k%29%3DP_%7Bik%7D%3D%5Cfrac%7BX_%7Bik%7D%7D%7BX_i%7D)

所以， ![[公式]](https://www.zhihu.com/equation?tex=w_i%5ET%5Ctilde+w_k+%3D+log%28P_%7Bik%7D%29+%3D+log%28X_%7Bik%7D%29-log%28X_i%29)

注意其中 ![[公式]](https://www.zhihu.com/equation?tex=log%28X_i%29) 破坏了交换 ![[公式]](https://www.zhihu.com/equation?tex=w%5Cleftrightarrow+%5Ctilde+w) 与 ![[公式]](https://www.zhihu.com/equation?tex=X%5Cleftrightarrow+X%5ET) 时的对称性，但是这一项并不依赖于 ![[公式]](https://www.zhihu.com/equation?tex=k) ，所以我们可以将其融合进关于 ![[公式]](https://www.zhihu.com/equation?tex=w_i) 的**bias项** ![[公式]](https://www.zhihu.com/equation?tex=b_i) ，第二部就是为了**平衡对称性**，我们再加入关于 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde+w_k) 的bias项 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde+b_k) ，我们就可以得到 ![[公式]](https://www.zhihu.com/equation?tex=w_i%5ET%5Ctilde+w_k+%2B+b_i+%2B+%5Ctilde+b_k+%3D+log%28X_%7Bik%7D%29) 的形式。

另一方面作者注意到模型的一个缺点是对于所有的co-occurence的权重是一样的，即使是那些较少发生的co-occurrence。作者认为这些可能是噪声，所以他加入了前面的 ![[公式]](https://www.zhihu.com/equation?tex=f%28X_%7Bij%7D%29) 项来做**weighted least squares regression**模型，即为

![[公式]](https://www.zhihu.com/equation?tex=J%3D%5Csum_%7Bi%2Cj%3D1%7D%5EVf%28X_%7Bij%7D%29%28w_i%5ET%5Ctilde+w_j+%2B+b_i+%2B+%5Ctilde+b_j+-logX_%7Bij%7D%29%5E2) 的形式。

其中**权重项** ![[公式]](https://www.zhihu.com/equation?tex=f) 需满足一下条件：

1. ![[公式]](https://www.zhihu.com/equation?tex=f%280%29%3D0) ，因为要求 ![[公式]](https://www.zhihu.com/equation?tex=lim_%7Bx%5Crightarrow+0%7Df%28x%29log%5E2x) 是有限的。
2. 较少发生的co-occurrence所占比重较小。
3. 对于较多发生的co-occurrence， ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) 也不能过大。

作者试验效果较好的权重函数形式是
$$
f(x)=\begin{cases} (x/x_max)^{\alpha} & \text{if }x<x_{max}\\1&\text{otherwise} \end{cases}
$$
![img](https://pic1.zhimg.com/80/v2-df9c9c0c6ed72525a197182161706c64_hd.jpg)

### 评估词向量

NLP的一般评估方法：Intrinsic vs extrinsic

* Intrinsic(内在)：
  * 评估特定或者中间子任务
  * 计算速度
  * 有助于理解该系统
  * 不清楚是否真的有用，除非与实际任务相关
* Extrinsic(外在):
  * 对实际任务的评估
  * 计算精度可能需要很长时间
  * 不清楚子系统是问题还是它的相互作用或其他子系统
  * 如果将一个子系统恰好替换为另一个子系统可以提高准确性

#### 词向量的内在评估

##### 词向量的类比

![image-20200220221743727](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220221743727.png)

* 通过加法后的余弦距离捕获直觉的语义和句法类比问题来评估单词向量，从而丢弃搜索中的输入单词！
* 问题：如果存在相关信息但不是线性的，该怎么办？

##### Glove 可视化

![image-20200220222842650](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220222842650.png)

从图中可以看出man与woman的向量和sir与madam之间的向量类似 平行

#### 类比评估和超参数

![image-20200220223525090](/Users/pengshiyuan/Library/Application Support/typora-user-images/image-20200220223525090.png)

* **词向量维度在300左右比较好**
* 非对称上下文（仅左侧单词）效果不佳
* 但这对于下游任务可能有所不同
* 对于Glove向量而言，**中心单词附近的窗口大小为8比较适合**

#### Idea

#####  通过全局上下文和多个单词原型改进单词表示

对单词旁的的单词窗口进行聚类，并用分配给多个不同聚类bank1，bank2等的每个单词进行重新训练

##### 词义的线性代数结构及其在多义性中的应用

* 来自诸如word2vec等标准词嵌入中的单词不同含义的加权线性和
* Exp:$v_{pike}=\alpha_1 v_{pike_1}+\alpha_2v_{pike_2}+\alpha_3 v_{pike_3}$
* $\alpha_1=\cfrac{f1}{f1+f2+f3}$  f:出现的频率
* 由于来自稀疏编码的想法，您实际上可以分离出不同的含义（假设它们相对普遍）

#### 外在的词向量评估

* 此类中的所有后续任务
* 一个好的单词向量应直接帮助的示例：
  * 命名实体识别：找到一个人，组织或位置

**目前对模型的评估基本都是内在的**



## 相关资料 

cs224n笔记(二)-[知乎-川陀学者](https://zhuanlan.zhihu.com/p/60208480)

[Adam那么棒，为什么还对SGD念念不忘]([http://wulc.me/2019/03/18/Adam%E9%82%A3%E4%B9%88%E6%A3%92%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E5%AF%B9SGD%E5%BF%B5%E5%BF%B5%E4%B8%8D%E5%BF%98/](http://wulc.me/2019/03/18/Adam那么棒，为什么还对SGD念念不忘/))



